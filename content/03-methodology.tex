% Local IspellDict: en
\section{Methodology}\label{ch03:method}

\subsection{Experimental Design} 

\figure[b]
    \centering
    \includegraphics[width=\textwidth]{figures/pipline_final.png}
    \caption{Overview of our proposed prototype for saliency object detection using the eye-tracking data and SAM3.}
    \label{fig:pipeline}
\endfigure

Our prototype implements a pipeline that transforms raw eye-tracking data into semantically meaningful object segmentations (see Figure \ref{fig:pipeline}). The approach is motivated by the hypothesis that human visual attention, as captured through fixations, provides a reliable signal for identifying salient objects in social media images. The pipeline consists of four main stages: First, we extract fixations from the raw 60 Hz eye-tracking measurements to obtain stable representations of visual attention. Second, we apply DBSCAN clustering to group spatially proximate fixations, identifying distinct regions of interest regardless of their shape. Third, we compute duration-weighted centroid points for each cluster to serve as location estimates for salient objects. Finally, we use these saliency points as prompts for SAM3, a state-of-the-art segmentation model, to extract precise object boundaries. This design leverages actual human visual behavior rather than computational saliency predictions, potentially offering more ecologically valid results for salient object detection in real-world social media contexts.

\textbf{Fixation Extraction.} The raw eye-tracking data consists of gaze position measurements recorded at 60 Hz, capturing where participants looked throughout their viewing session. However, these high-frequency measurements contain substantial noise from micro-saccades, measurement errors, and brief involuntary eye movements. To obtain a more meaningful representation of visual attention, we extract fixations, which are stable periods where gaze remains on a specific location. Fixations represent deliberate visual processing rather than transitional eye movements, making them a more appropriate input for identifying salient regions. This preprocessing step effectively filters noise while preserving the spatial and temporal characteristics of participant attention.

\textbf{Fixation Clustering.} Having extracted stable fixation points, we next identify spatially coherent regions of attention, we apply DBSCAN \newpage (Density-Based Spatial Clustering of Applications with Noise) to group spatially proximate fixations. DBSCAN is particularly well-suited for this task because it can discover clusters of arbitrary shape without requiring a predetermined number of clusters. This flexibility is crucial for social media images, where salient objects vary considerably in size, shape, and spatial distribution. Unlike centroid-based clustering methods such as k-means, DBSCAN can effectively capture elongated or irregularly shaped attention patterns that may correspond to complex objects or multiple nearby objects of interest. Additionally, DBSCAN's noise detection mechanism allows us to distinguish between sustained attention on specific regions and scattered, less meaningful fixations \citep{ester_density-based_1996}.

\textbf{Saliency Point Generation.} From the clustered fixations, we generate point-based location estimates to represent salient regions in the image. We consider two sources for these saliency points: first, all clusters identified by DBSCAN, and second, noise points (fixations not assigned to any cluster) that exceed a fixation duration threshold of 1 second. This threshold ensures that even isolated but sustained attention is captured as potentially salient, though we acknowledge this value is preliminary and may require adjustment based on empirical evaluation. For each cluster, we compute a duration weighted centroid, where the position of each fixation is weighted by how long participants attended to that location. This weighting scheme ensures that areas receiving more sustained attention exert greater influence on the final saliency point position, better reflecting the perceived importance of different regions within a cluster. While this centroid-based approach provides a single representative point per salient region, it does sacrifice information about the spatial extent and shape of the attention distribution, which may be relevant for irregularly shaped objects or diffuse attention patterns.

\textbf{Object Segmentation.} To extract precise object boundaries from the identified saliency points, we employ SAM3 (Segment Anything Model 3), a state-of-the-art segmentation model that supports prompt-based inference \citep{carion_sam_2025}. The choice of SAM3 is motivated by two key requirements: first, the need for high-quality segmentation performance comparable to current best practices in the field, and second, the ability to accept point prompts that guide the segmentation process. SAM3's architecture is specifically designed for promptable segmentation tasks, making it well-suited for our zero-shot approach where saliency points serve as spatial prompts indicating regions of interest. By providing our computed saliency points as input prompts, SAM3 generates segmentation masks that delineate the boundaries of objects receiving visual attention, effectively translating human gaze behavior into structured object-level representations without requiring task-specific training or fine-tuning.


\subsection{Planned Practical Steps}

The planned next steps can be divided into two main parts. The first part focuses on the optimization of the saliency object detection process, while the second part 
focuses on the extraction of insights from the segmented objects, based on an exploratory data analysis \citep{tukey_exploratory_1977}.

As discussed in the previous section, the current approach offers several options for improvement. These include image preprocessing, the adjustment of threshold values, 
the use of box prompts instead of point prompts, and the exploration of additional clustering methods such as k means. Moreover, it is possible to add further steps between 
the main stages of the workflow in order to create more stable and consistent results. As a last possibility, the model can be fine-tuned, using existing scientific datasets such as
WXSOD or PASCAL-S \citep{quan_wxsod_2025, ccvl_pascal-s_2018}, which already include saliency masks. The overall goal of this first step is to produce the best possible saliency masks for social 
media images. Better masks allow for a more reliable extraction of insights in the next step.

After the optimization step, the quality of the segmented masks must be evaluated. This evaluation includes three main questions. First, it must be examined if the masks truly 
represent the salient parts of the image. Second, the mask quality must be assessed by measuring how well the masks fit the expected important regions. Third, it must be 
analyzed whether certain types of images work better or worse with saliency based segmentation. This helps identify strengths and limitations of the approach for different 
kinds of social media content.

Furthermore, the generated masks allow for additional exploratory fields of analysis that can be grouped into three areas: user insights, content insights, and accessibility support. The user 
related area includes predictive user profiling, where salient regions help identify which visual elements attract individual users. The content related area includes a deeper 
understanding of social media images, the detection of clickbait content, and guidance for creators who work with under optimized images. The accessibility related area focuses 
on focus aware alternative text generation, where the masks help identify the most important visual elements for users with visual impairments. Together, these groups show the 
wider potential of the approach beyond the core segmentation step.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main_thesis"
%%% TeX-command-extra-options: "-shell-escape"
%%% End:
