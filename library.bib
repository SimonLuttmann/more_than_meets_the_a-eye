
@article{peffers_design_2008,
	title = {A {Design} {Science} {Research} {Methodology} for {Information} {Systems} {Research}},
	volume = {24},
	abstract = {The paper motivates, presents, demonstrates in use, and evaluates a methodology for conducting design science (DS) research in information systems (IS). DS is of importance in a discipline oriented to the creation of successful artifacts. Several researchers have pioneered DS research in IS, yet over the past 15 years, little DS research has been done within the discipline. The lack of a methodology to serve as a commonly accepted framework for DS research and of a template for its presentation may have contributed to its slow adoption. The design science research methodology (DSRM) presented here incorporates principles, practices, and procedures required to carry out such research and meets three objectives: it is consistent with prior literature, it provides a nominal process model for doing DS research, and it provides a mental model for presenting and evaluating DS research in IS. The DS process includes six steps: problem identification and motivation, definition of the objectives for a solution, design and development, demonstration, evaluation, and communication. We demonstrate and evaluate the methodology by presenting four case studies in terms of the DSRM, including cases that present the design of a database to support health assessment methods, a software reuse measure, an Internet video telephony application, and an IS planning method. The designed methodology effectively satisfies the three objectives and has the potential to help aid the acceptance of DS research in the IS discipline.},
	language = {en},
	number = {3},
	journal = {Journal of Management Information Systems},
	author = {Peffers, Ken and Tuunanen, Tuure and Rothenberger, Marcus A. and Chatterjee, Samir},
	year = {2008},
	keywords = {bib hinzugefügt},
	pages = {45--77},
	file = {Peffers et al. - 2008 - A Design Science Research Methodology for Informat.pdf:/Users/annikaterhoerst/Zotero/storage/S5W885M3/Peffers et al. - 2008 - A Design Science Research Methodology for Informat.pdf:application/pdf},
}

@article{minaee_image_2022,
	title = {Image {Segmentation} {Using} {Deep} {Learning}: {A} {Survey}},
	volume = {44},
	issn = {1939-3539},
	shorttitle = {Image {Segmentation} {Using} {Deep} {Learning}},
	url = {https://ieeexplore.ieee.org/abstract/document/9356353},
	doi = {10.1109/TPAMI.2021.3059968},
	abstract = {Image segmentation is a key task in computer vision and image processing with important applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among others, and numerous segmentation algorithms are found in the literature. Against this backdrop, the broad success of deep learning (DL) has prompted the development of new image segmentation approaches leveraging DL models. We provide a comprehensive review of this recent literature, covering the spectrum of pioneering efforts in semantic and instance segmentation, including convolutional pixel-labeling networks, encoder-decoder architectures, multiscale and pyramid-based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the relationships, strengths, and challenges of these DL-based segmentation models, examine the widely used datasets, compare performances, and discuss promising research directions.},
	number = {7},
	urldate = {2025-11-15},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Minaee, Shervin and Boykov, Yuri and Porikli, Fatih and Plaza, Antonio and Kehtarnavaz, Nasser and Terzopoulos, Demetri},
	month = jul,
	year = {2022},
	keywords = {deep learning, Deep learning, Computational modeling, Semantics, Computer architecture, convolutional neural networks, encoder-decoder models, Generative adversarial networks, generative models, Image segmentation, instance segmentation, Logic gates, medical image segmentation, panoptic segmentation, recurrent models, semantic segmentation},
	pages = {3523--3542},
	file = {Full Text PDF:/Users/annikaterhoerst/Zotero/storage/TFSDIHR8/Minaee et al. - 2022 - Image Segmentation Using Deep Learning A Survey.pdf:application/pdf},
}

@article{yu_techniques_2023,
	title = {Techniques and {Challenges} of {Image} {Segmentation}: {A} {Review}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	shorttitle = {Techniques and {Challenges} of {Image} {Segmentation}},
	url = {https://www.mdpi.com/2079-9292/12/5/1199},
	doi = {10.3390/electronics12051199},
	abstract = {Image segmentation, which has become a research hotspot in the field of image processing and computer vision, refers to the process of dividing an image into meaningful and non-overlapping regions, and it is an essential step in natural scene understanding. Despite decades of effort and many achievements, there are still challenges in feature extraction and model design. In this paper, we review the advancement in image segmentation methods systematically. According to the segmentation principles and image data characteristics, three important stages of image segmentation are mainly reviewed, which are classic segmentation, collaborative segmentation, and semantic segmentation based on deep learning. We elaborate on the main algorithms and key techniques in each stage, compare, and summarize the advantages and defects of different segmentation models, and discuss their applicability. Finally, we analyze the main challenges and development trends of image segmentation techniques.},
	language = {en},
	number = {5},
	urldate = {2025-11-16},
	journal = {Electronics},
	author = {Yu, Ying and Wang, Chunping and Fu, Qiang and Kou, Renke and Huang, Fuyu and Yang, Boxiong and Yang, Tingting and Gao, Mingliang},
	month = jan,
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning, semantic segmentation, co-segmentation, image processing, image segmentation},
	pages = {1199},
	file = {Full Text PDF:/Users/annikaterhoerst/Zotero/storage/6DEFPTAL/Yu et al. - 2023 - Techniques and Challenges of Image Segmentation A Review.pdf:application/pdf},
}

@article{dhanachandra_image_2015,
	series = {Eleventh {International} {Conference} on {Communication} {Networks}, {ICCN} 2015, {August} 21-23, 2015, {Bangalore}, {India} {Eleventh} {International} {Conference} on {Data} {Mining} and {Warehousing}, {ICDMW} 2015, {August} 21-23, 2015, {Bangalore}, {India} {Eleventh} {International} {Conference} on {Image} and {Signal} {Processing}, {ICISP} 2015, {August} 21-23, 2015, {Bangalore}, {India}},
	title = {Image {Segmentation} {Using} \textit{{K}} -means {Clustering} {Algorithm} and {Subtractive} {Clustering} {Algorithm}},
	volume = {54},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050915014143},
	doi = {10.1016/j.procs.2015.06.090},
	abstract = {Image segmentation is the classification of an image into different groups. Many researches have been done in the area of image segmentation using clustering. There are different methods and one of the most popular methods is k-means clustering algorithm. K -means clustering algorithm is an unsupervised algorithm and it is used to segment the interest area from the background. But before applying K -means algorithm, first partial stretching enhancement is applied to the image to improve the quality of the image. Subtractive clustering method is data clustering method where it generates the centroid based on the potential value of the data points. So subtractive cluster is used to generate the initial centers and these centers are used in k-means algorithm for the segmentation of image. Then finally medial filter is applied to the segmented image to remove any unwanted region from the image.},
	urldate = {2025-11-16},
	journal = {Procedia Computer Science},
	author = {Dhanachandra, Nameirakpam and Manglem, Khumanthem and Chanu, Yambem Jina},
	month = jan,
	year = {2015},
	keywords = {Image segmentation, -means clustering, Median filter, Partial contrast stretching, Subtractive clustering.},
	pages = {764--771},
	file = {ScienceDirect Full Text PDF:/Users/annikaterhoerst/Zotero/storage/J7ERFPFQ/Dhanachandra et al. - 2015 - Image Segmentation Using K -means Clustering Algorithm and Subtractive Clustering Algorithm.pdf:application/pdf;ScienceDirect Snapshot:/Users/annikaterhoerst/Zotero/storage/J2NQRRXG/S1877050915014143.html:text/html},
}

@inproceedings{kirillov_panoptic_2019,
	address = {Long Beach, CA, USA},
	title = {Panoptic {Segmentation}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-7281-3293-8},
	url = {https://ieeexplore.ieee.org/document/8953237/},
	doi = {10.1109/CVPR.2019.00963},
	abstract = {We propose and study a task we name panoptic segmentation (PS). Panoptic segmentation uniﬁes the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and uniﬁed manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more uniﬁed view of image segmentation. For more analysis and up-todate results, please check the arXiv version of the paper: https://arxiv.org/abs/1801.00868.},
	language = {en},
	urldate = {2025-11-16},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Dollár, Piotr},
	month = jun,
	year = {2019},
	pages = {9396--9405},
	file = {PDF:/Users/annikaterhoerst/Zotero/storage/J876395N/Kirillov et al. - 2019 - Panoptic Segmentation.pdf:application/pdf},
}

@misc{li_blip_2022,
	title = {{BLIP}: {Bootstrapping} {Language}-{Image} {Pre}-training for {Unified} {Vision}-{Language} {Understanding} and {Generation}},
	shorttitle = {{BLIP}},
	url = {http://arxiv.org/abs/2201.12086},
	doi = {10.48550/arXiv.2201.12086},
	abstract = {Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7\% in average recall@1), image captioning (+2.8\% in CIDEr), and VQA (+1.6\% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at https://github.com/salesforce/BLIP.},
	urldate = {2025-11-16},
	publisher = {arXiv},
	author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
	month = feb,
	year = {2022},
	note = {arXiv:2201.12086 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/annikaterhoerst/Zotero/storage/GC8E4NSB/Li et al. - 2022 - BLIP Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Genera.pdf:application/pdf;Snapshot:/Users/annikaterhoerst/Zotero/storage/TE4JBZBK/2201.html:text/html},
}

@misc{ahmadi_context_2018,
	title = {Context aware saliency map generation using semantic segmentation},
	url = {http://arxiv.org/abs/1801.00256},
	doi = {10.48550/arXiv.1801.00256},
	abstract = {Saliency map detection, as a method for detecting important regions of an image, is used in many applications such as image classification and recognition. We propose that context detection could have an essential role in image saliency detection. This requires extraction of high level features. In this paper a saliency map is proposed, based on image context detection using semantic segmentation as a high level feature. Saliency map from semantic information is fused with color and contrast based saliency maps. The final saliency map is then generated. Simulation results for Pascal-voc11 image dataset show 99\% accuracy in context detection. Also final saliency map produced by our proposed method shows acceptable results in detecting salient points.},
	urldate = {2025-11-28},
	publisher = {arXiv},
	author = {Ahmadi, Mahdi and Hajabdollahi, Mohsen and Karimi, Nader and Samavi, Shadrokh},
	month = jan,
	year = {2018},
	note = {arXiv:1801.00256 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:/Users/annikaterhoerst/Zotero/storage/GAXEENLH/Ahmadi et al. - 2018 - Context aware saliency map generation using semantic segmentation.pdf:application/pdf;Snapshot:/Users/annikaterhoerst/Zotero/storage/YGS2Y83Q/1801.html:text/html},
}

@inproceedings{yuan_saliencyclip-sam_2026,
	address = {Singapore},
	title = {{SaliencyCLIP}-{SAM}: {Bridging} {Text} and {Image} {Towards} {Text}-{Driven} {Salient} {Object} {Detection}},
	isbn = {9789819533930},
	shorttitle = {{SaliencyCLIP}-{SAM}},
	url = {https://link.springer.com/chapter/10.1007/978-981-95-3393-0_3},
	doi = {10.1007/978-981-95-3393-0_3},
	abstract = {Many unsupervised salient object detection methods rely heavily on handcraft visual priors. Existing deep learning-based models require task-specific training and high annotation costs, limiting their generalization to complex scenes. In this paper, we propose a text-driven salient object detection framework by innovatively integrating the rich visual-language semantic information of Contrastive Language-Image Pre-training (CLIP) and the segmentation power of the Segment Anything Model (SAM), which is without explicit task-specific training and manually labeling. Specifically, to mitigate the negative impact of some ‘global’ patches in the final visual feature from the CLIP visual encoder, we propose a Multi-level Self Cosine-Similarity Correction model (MSCC), which calculates the cosine similarities of multi-level visual features for enhancing the local semantic correlation in saliency regions. With the modified final visual feature, we derive coarse salient regions. Then, we introduce a Multi-level Saliency Mask Refinement model, where coarse saliency maps from CLIP generate diverse prompt constraints (points, boxes, masks) for SAM, resulting in multi-level fine-grained saliency masks without manual intervention. Experimental results on public salient object benchmarks demonstrate the effectiveness of the proposed text-driven framework in segmenting salient objects, which provides empirical insights and key breakthroughs for leveraging foundation models in perceptual tasks through text prompt-based methods.},
	language = {en},
	booktitle = {Image and {Graphics}},
	publisher = {Springer Nature},
	author = {Yuan, Ying and Zhang, Yingying and Zhang, Shuai and Wang, Hongjuan},
	editor = {Lin, Zhouchen and Wang, Liang and Jiang, Yugang and Wang, Xuesong and Liao, Shengcai and Shan, Shiguang and Liu, Risheng and Dong, Jing and Yu, Xin},
	year = {2026},
	keywords = {CLIP, Salient Object Detection, SAM, Self Cosine-Similarity},
	pages = {29--41},
	file = {Full Text PDF:/Users/annikaterhoerst/Zotero/storage/MDS73A8N/Yuan et al. - 2026 - SaliencyCLIP-SAM Bridging Text and Image Towards Text-Driven Salient Object Detection.pdf:application/pdf},
}

@misc{chang_marine_2025,
	title = {Marine {Saliency} {Segmenter}: {Object}-{Focused} {Conditional} {Diffusion} with {Region}-{Level} {Semantic} {Knowledge} {Distillation}},
	shorttitle = {Marine {Saliency} {Segmenter}},
	url = {http://arxiv.org/abs/2504.02391},
	doi = {10.48550/arXiv.2504.02391},
	abstract = {Marine Saliency Segmentation (MSS) plays a pivotal role in various vision-based marine exploration tasks. However, existing marine segmentation techniques face the dilemma of object mislocalization and imprecise boundaries due to the complex underwater environment. Meanwhile, despite the impressive performance of diffusion models in visual segmentation, there remains potential to further leverage contextual semantics to enhance feature learning of region-level salient objects, thereby improving segmentation outcomes. Building on this insight, we propose DiffMSS, a novel marine saliency segmenter based on the diffusion model, which utilizes semantic knowledge distillation to guide the segmentation of marine salient objects. Specifically, we design a region-word similarity matching mechanism to identify salient terms at the word level from the text descriptions. These high-level semantic features guide the conditional feature learning network in generating salient and accurate diffusion conditions with semantic knowledge distillation. To further refine the segmentation of fine-grained structures in unique marine organisms, we develop the dedicated consensus deterministic sampling to suppress overconfident missegmentations. Comprehensive experiments demonstrate the superior performance of DiffMSS over state-of-the-art methods in both quantitative and qualitative evaluations.},
	urldate = {2025-11-29},
	publisher = {arXiv},
	author = {Chang, Laibin and Wang, Yunke and Huang, JiaXing and Deng, Longxiang and Du, Bo and Xu, Chang},
	month = jun,
	year = {2025},
	note = {arXiv:2504.02391 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:/Users/annikaterhoerst/Zotero/storage/3KXSSYMN/Chang et al. - 2025 - Marine Saliency Segmenter Object-Focused Conditional Diffusion with Region-Level Semantic Knowledge.pdf:application/pdf;Snapshot:/Users/annikaterhoerst/Zotero/storage/FYUYTKU7/2504.html:text/html},
}

@inproceedings{kakanopas_unified_2021,
	address = {Cham},
	title = {A {Unified} {Framework} for {Saliency} {Segmentation}},
	isbn = {978-3-030-79757-7},
	doi = {10.1007/978-3-030-79757-7_19},
	abstract = {Saliency segmentation is an extendable saliency detection that can detect and segment the most interested object(s) of an image. During the past decade, many saliency detection methods have been proposed. Those methods can correctly identify salient locations, but they provide the detected saliency maps with a diversity of intensity values, highlighting only high contrast edges, unevenly salient regions, and ill-defined salient boundaries, thus making them unextendible to saliency segmentation. Therefore, this paper proposes a unified framework for saliency segmentation. This framework consists of three main processes: (i) saliency feature extraction, implemented with a set of i-Hola filters, our recently proposed method, (ii) saliency map selection, using a multi-solution technique in selecting the optimal saliency map, and (iii) saliency segmentation, implemented with an iterative approach. Based on a challenging dataset divided into seven categories with different characteristics, the experimental results showed that our proposed method outperformed the baselines in almost all categories in terms of IOU performance.},
	language = {en},
	booktitle = {Recent {Advances} in {Information} and {Communication} {Technology} 2021},
	publisher = {Springer International Publishing},
	author = {Kakanopas, Donyarut and Woraratpanya, Kuntpong},
	editor = {Meesad, Phayung and Sodsee, Dr. Sunantha and Jitsakul, Watchareewan and Tangwannawit, Sakchai},
	year = {2021},
	keywords = {Hola filter, Saliency detection, Saliency map, Saliency segmentation, Segmented saliency},
	pages = {191--200},
	file = {Full Text PDF:/Users/annikaterhoerst/Zotero/storage/4HXA6UVH/Kakanopas und Woraratpanya - 2021 - A Unified Framework for Saliency Segmentation.pdf:application/pdf},
}

@misc{kim_rethinking_2024,
	title = {Rethinking {Saliency}-{Guided} {Weakly}-{Supervised} {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/2404.00918},
	doi = {10.48550/arXiv.2404.00918},
	abstract = {This paper presents a fresh perspective on the role of saliency maps in weakly-supervised semantic segmentation (WSSS) and offers new insights and research directions based on our empirical findings. We conduct comprehensive experiments and observe that the quality of the saliency map is a critical factor in saliency-guided WSSS approaches. Nonetheless, we find that the saliency maps used in previous works are often arbitrarily chosen, despite their significant impact on WSSS. Additionally, we observe that the choice of the threshold, which has received less attention before, is non-trivial in WSSS. To facilitate more meaningful and rigorous research for saliency-guided WSSS, we introduce {\textbackslash}texttt\{WSSS-BED\}, a standardized framework for conducting research under unified conditions. {\textbackslash}texttt\{WSSS-BED\} provides various saliency maps and activation maps for seven WSSS methods, as well as saliency maps from unsupervised salient object detection models.},
	urldate = {2025-11-29},
	publisher = {arXiv},
	author = {Kim, Beomyoung and Kim, Donghyun and Hwang, Sung Ju},
	month = apr,
	year = {2024},
	note = {arXiv:2404.00918 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:/Users/annikaterhoerst/Zotero/storage/AILSSTQN/Kim et al. - 2024 - Rethinking Saliency-Guided Weakly-Supervised Semantic Segmentation.pdf:application/pdf;Snapshot:/Users/annikaterhoerst/Zotero/storage/Z52KPT9N/2404.html:text/html},
}

@misc{zhang_dualgazenet_2025,
	title = {{DualGazeNet}: {A} {Biologically} {Inspired} {Dual}-{Gaze} {Query} {Network} for {Salient} {Object} {Detection}},
	shorttitle = {{DualGazeNet}},
	url = {http://arxiv.org/abs/2511.18865},
	doi = {10.48550/arXiv.2511.18865},
	abstract = {Recent salient object detection (SOD) methods aim to improve performance in four key directions: semantic enhancement, boundary refinement, auxiliary task supervision, and multi-modal fusion. In pursuit of continuous gains, these approaches have evolved toward increasingly sophisticated architectures with multi-stage pipelines, specialized fusion modules, edge-guided learning, and elaborate attention mechanisms. However, this complexity paradoxically introduces feature redundancy and cross-component interference that obscure salient cues, ultimately reaching performance bottlenecks. In contrast, human vision achieves efficient salient object identification without such architectural complexity. This contrast raises a fundamental question: can we design a biologically grounded yet architecturally simple SOD framework that dispenses with most of this engineering complexity, while achieving state-of-the-art accuracy, computational efficiency, and interpretability? In this work, we answer this question affirmatively by introducing DualGazeNet, a biologically inspired pure Transformer framework that models the dual biological principles of robust representation learning and magnocellular-parvocellular dual-pathway processing with cortical attention modulation in the human visual system. Extensive experiments on five RGB SOD benchmarks show that DualGazeNet consistently surpasses 25 state-of-the-art CNN- and Transformer-based methods. On average, DualGazeNet achieves about 60{\textbackslash}\% higher inference speed and 53.4{\textbackslash}\% fewer FLOPs than four Transformer-based baselines of similar capacity (VST++, MDSAM, Sam2unet, and BiRefNet). Moreover, DualGazeNet exhibits strong cross-domain generalization, achieving leading or highly competitive performance on camouflaged and underwater SOD benchmarks without relying on additional modalities.},
	urldate = {2025-11-30},
	publisher = {arXiv},
	author = {Zhang, Yu and Ping, Haoan and Li, Yuchen and Bing, Zhenshan and Sun, Fuchun and Knoll, Alois},
	month = nov,
	year = {2025},
	note = {arXiv:2511.18865 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:/Users/annikaterhoerst/Zotero/storage/VXMWNPGU/Zhang et al. - 2025 - DualGazeNet A Biologically Inspired Dual-Gaze Query Network for Salient Object Detection.pdf:application/pdf;Snapshot:/Users/annikaterhoerst/Zotero/storage/5XQKM7RX/2511.html:text/html},
}

@misc{grossman_using_2025,
	title = {Using {Salient} {Object} {Detection} to {Identify} {Manipulative} {Cookie} {Banners} that {Circumvent} {GDPR}},
	url = {http://arxiv.org/abs/2510.26967},
	doi = {10.48550/arXiv.2510.26967},
	abstract = {The main goal of this paper is to study how often cookie banners that comply with the General Data Protection Regulation (GDPR) contain aesthetic manipulation, a design tactic to draw users' attention to the button that permits personal data sharing. As a byproduct of this goal, we also evaluate how frequently the banners comply with GDPR and the recommendations of national data protection authorities regarding banner designs. We visited 2,579 websites and identified the type of cookie banner implemented. Although 45\% of the relevant websites have fully compliant banners, we found aesthetic manipulation on 38\% of the compliant banners. Unlike prior studies of aesthetic manipulation, we use a computer vision model for salient object detection to measure how salient (i.e., attention-drawing) each banner element is. This enables the discovery of new types of aesthetic manipulation (e.g., button placement), and leads us to conclude that aesthetic manipulation is more common than previously reported (38\% vs 27\% of banners). To study the effects of user and/or website location on cookie banner design, we include websites within the European Union (EU), where privacy regulation enforcement is more stringent, and websites outside the EU. We visited websites from IP addresses in the EU and from IP addresses in the United States (US). We find that 13.9\% of EU websites change their banner design when the user is from the US, and EU websites are roughly 48.3\% more likely to use aesthetic manipulation than non-EU websites, highlighting their innovative responses to privacy regulation.},
	urldate = {2025-11-30},
	publisher = {arXiv},
	author = {Grossman, Riley and Smith, Michael and Borcea, Cristian and Chen, Yi},
	month = oct,
	year = {2025},
	note = {arXiv:2510.26967 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
	file = {Full Text PDF:/Users/annikaterhoerst/Zotero/storage/UPWPNVIV/Grossman et al. - 2025 - Using Salient Object Detection to Identify Manipulative Cookie Banners that Circumvent GDPR.pdf:application/pdf;Snapshot:/Users/annikaterhoerst/Zotero/storage/MRG7QT7Z/2510.html:text/html},
}

@article{liu_learning_2011,
	title = {Learning to {Detect} a {Salient} {Object}},
	volume = {33},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/abstract/document/5432215},
	doi = {10.1109/TPAMI.2010.70},
	abstract = {In this paper, we study the salient object detection problem for images. We formulate this problem as a binary labeling task where we separate the salient object from the background. We propose a set of novel features, including multiscale contrast, center-surround histogram, and color spatial distribution, to describe a salient object locally, regionally, and globally. A conditional random field is learned to effectively combine these features for salient object detection. Further, we extend the proposed approach to detect a salient object from sequential images by introducing the dynamic salient features. We collected a large image database containing tens of thousands of carefully labeled images by multiple users and a video segment database, and conducted a set of experiments over them to demonstrate the effectiveness of the proposed approach.},
	number = {2},
	urldate = {2025-12-02},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Liu, Tie and Yuan, Zejian and Sun, Jian and Wang, Jingdong and Zheng, Nanning and Tang, Xiaoou and Shum, Heung-Yeung},
	month = feb,
	year = {2011},
	keywords = {conditional random field, Histograms, Humans, Image databases, Image segmentation, Labeling, Object detection, saliency map., Salient object detection, Spatial databases, Sun, visual attention, Visual databases},
	pages = {353--367},
	file = {Full Text PDF:/Users/annikaterhoerst/Zotero/storage/7XNXSSH3/Liu et al. - 2011 - Learning to Detect a Salient Object.pdf:application/pdf},
}

@misc{xiong_sam3-unet_2025,
	title = {{SAM3}-{UNet}: {Simplified} {Adaptation} of {Segment} {Anything} {Model} 3},
	shorttitle = {{SAM3}-{UNet}},
	url = {http://arxiv.org/abs/2512.01789},
	doi = {10.48550/arXiv.2512.01789},
	abstract = {In this paper, we introduce SAM3-UNet, a simplified variant of Segment Anything Model 3 (SAM3), designed to adapt SAM3 for downstream tasks at a low cost. Our SAM3-UNet consists of three components: a SAM3 image encoder, a simple adapter for parameter-efficient fine-tuning, and a lightweight U-Net-style decoder. Preliminary experiments on multiple tasks, such as mirror detection and salient object detection, demonstrate that the proposed SAM3-UNet outperforms the prior SAM2-UNet and other state-of-the-art methods, while requiring less than 6 GB of GPU memory during training with a batch size of 12. The code is publicly available at https://github.com/WZH0120/SAM3-UNet.},
	urldate = {2025-12-02},
	publisher = {arXiv},
	author = {Xiong, Xinyu and Wu, Zihuang and Lu, Lei and Xia, Yufa},
	month = dec,
	year = {2025},
	note = {arXiv:2512.01789 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:/Users/annikaterhoerst/Zotero/storage/BQUUAPSK/Xiong et al. - 2025 - SAM3-UNet Simplified Adaptation of Segment Anything Model 3.pdf:application/pdf;Snapshot:/Users/annikaterhoerst/Zotero/storage/I9YT8GCM/2512.html:text/html},
}

@article{borji_salient_2019,
	title = {Salient object detection: {A} survey},
	volume = {5},
	issn = {2096-0662},
	shorttitle = {Salient object detection},
	url = {https://ieeexplore.ieee.org/abstract/document/10897429},
	doi = {10.1007/s41095-019-0149-9},
	abstract = {Detecting and segmenting salient objects from natural scenes, often referred to as salient object detection, has attracted great interest in computer vision. While many models have been proposed and several applications have emerged, a deep understanding of achievements and issues remains lacking. We aim to provide a comprehensive review of recent progress in salient object detection and situate this field among other closely related areas such as generic scene segmentation, object proposal generation, and saliency for fixation prediction. Covering 228 publications, we survey i) roots, key concepts, and tasks, ii) core techniques and main modeling trends, and iii) datasets and evaluation metrics for salient object detection. We also discuss open problems such as evaluation metrics and dataset bias in model performance, and suggest future research directions.},
	number = {2},
	urldate = {2025-12-02},
	journal = {Computational Visual Media},
	author = {Borji, Ali and Cheng, Ming-Ming and Hou, Qibin and Jiang, Huaizu and Li, Jia},
	month = jun,
	year = {2019},
	keywords = {regions of interest, saliency, salient object detection, visual attention},
	pages = {117--150},
	file = {Full Text PDF:/Users/annikaterhoerst/Zotero/storage/BQQZ73FF/Borji et al. - 2019 - Salient object detection A survey.pdf:application/pdf},
}

@misc{liu_samsod_2025,
	title = {{SAMSOD}: {Rethinking} {SAM} {Optimization} for {RGB}-{T} {Salient} {Object} {Detection}},
	shorttitle = {{SAMSOD}},
	url = {http://arxiv.org/abs/2510.03689},
	doi = {10.48550/arXiv.2510.03689},
	abstract = {RGB-T salient object detection (SOD) aims to segment attractive objects by combining RGB and thermal infrared images. To enhance performance, the Segment Anything Model has been fine-tuned for this task. However, the imbalance convergence of two modalities and significant gradient difference between high- and low- activations are ignored, thereby leaving room for further performance enhancement. In this paper, we propose a model called {\textbackslash}textit\{SAMSOD\}, which utilizes unimodal supervision to enhance the learning of non-dominant modality and employs gradient deconfliction to reduce the impact of conflicting gradients on model convergence. The method also leverages two decoupled adapters to separately mask high- and low-activation neurons, emphasizing foreground objects by enhancing background learning. Fundamental experiments on RGB-T SOD benchmark datasets and generalizability experiments on scribble supervised RGB-T SOD, fully supervised RGB-D SOD datasets and full-supervised RGB-D rail surface defect detection all demonstrate the effectiveness of our proposed method.},
	urldate = {2025-12-02},
	publisher = {arXiv},
	author = {Liu, Zhengyi and Wang, Xinrui and Fang, Xianyong and Tu, Zhengzheng and Wang, Linbo},
	month = oct,
	year = {2025},
	note = {arXiv:2510.03689 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:/Users/annikaterhoerst/Zotero/storage/S9CBJSWP/Liu et al. - 2025 - SAMSOD Rethinking SAM Optimization for RGB-T Salient Object Detection.pdf:application/pdf;Snapshot:/Users/annikaterhoerst/Zotero/storage/R7N5IMSU/2510.html:text/html},
}

@misc{abdolahnejad_boundary_2023,
	title = {Boundary {Attention} {Mapping} ({BAM}): {Fine}-grained saliency maps for segmentation of {Burn} {Injuries}},
	shorttitle = {Boundary {Attention} {Mapping} ({BAM})},
	url = {http://arxiv.org/abs/2305.15365},
	doi = {10.48550/arXiv.2305.15365},
	abstract = {Burn injuries can result from mechanisms such as thermal, chemical, and electrical insults. A prompt and accurate assessment of burns is essential for deciding definitive clinical treatments. Currently, the primary approach for burn assessments, via visual and tactile observations, is approximately 60\%-80\% accurate. The gold standard is biopsy and a close second would be non-invasive methods like Laser Doppler Imaging (LDI) assessments, which have up to 97\% accuracy in predicting burn severity and the required healing time. In this paper, we introduce a machine learning pipeline for assessing burn severities and segmenting the regions of skin that are affected by burn. Segmenting 2D colour images of burns allows for the injured versus non-injured skin to be delineated, clearly marking the extent and boundaries of the localized burn/region-of-interest, even during remote monitoring of a burn patient. We trained a convolutional neural network (CNN) to classify four severities of burns. We built a saliency mapping method, Boundary Attention Mapping (BAM), that utilises this trained CNN for the purpose of accurately localizing and segmenting the burn regions from skin burn images. We demonstrated the effectiveness of our proposed pipeline through extensive experiments and evaluations using two datasets; 1) A larger skin burn image dataset consisting of 1684 skin burn images of four burn severities, 2) An LDI dataset that consists of a total of 184 skin burn images with their associated LDI scans. The CNN trained using the first dataset achieved an average F1-Score of 78\% and micro/macro- average ROC of 85\% in classifying the four burn severities. Moreover, a comparison between the BAM results and LDI results for measuring injury boundary showed that the segmentations generated by our method achieved 91.60\% accuracy, 78.17\% sensitivity, and 93.37\% specificity.},
	urldate = {2025-12-02},
	publisher = {arXiv},
	author = {Abdolahnejad, Mahla and Lee, Justin and Chan, Hannah and Morzycki, Alex and Ethier, Olivier and Mo, Anthea and Liu, Peter X. and Wong, Joshua N. and Hong, Colin and Joshi, Rakesh},
	month = may,
	year = {2023},
	note = {arXiv:2305.15365 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:/Users/annikaterhoerst/Zotero/storage/R6BHSFMX/Abdolahnejad et al. - 2023 - Boundary Attention Mapping (BAM) Fine-grained saliency maps for segmentation of Burn Injuries.pdf:application/pdf;Snapshot:/Users/annikaterhoerst/Zotero/storage/MRBV72TS/2305.html:text/html},
}

@misc{chen_sam3-adapter_2025,
	title = {{SAM3}-{Adapter}: {Efficient} {Adaptation} of {Segment} {Anything} 3 for {Camouflage} {Object} {Segmentation}, {Shadow} {Detection}, and {Medical} {Image} {Segmentation}},
	shorttitle = {{SAM3}-{Adapter}},
	url = {http://arxiv.org/abs/2511.19425},
	doi = {10.48550/arXiv.2511.19425},
	abstract = {The rapid rise of large-scale foundation models has reshaped the landscape of image segmentation, with models such as Segment Anything achieving unprecedented versatility across diverse vision tasks. However, previous generations-including SAM and its successor-still struggle with fine-grained, low-level segmentation challenges such as camouflaged object detection, medical image segmentation, cell image segmentation, and shadow detection. To address these limitations, we originally proposed SAM-Adapter in 2023, demonstrating substantial gains on these difficult scenarios. With the emergence of Segment Anything 3 (SAM3)-a more efficient and higher-performing evolution with a redesigned architecture and improved training pipeline-we revisit these long-standing challenges. In this work, we present SAM3-Adapter, the first adapter framework tailored for SAM3 that unlocks its full segmentation capability. SAM3-Adapter not only reduces computational overhead but also consistently surpasses both SAM and SAM2-based solutions, establishing new state-of-the-art results across multiple downstream tasks, including medical imaging, camouflaged (concealed) object segmentation, and shadow detection. Built upon the modular and composable design philosophy of the original SAM-Adapter, SAM3-Adapter provides stronger generalizability, richer task adaptability, and significantly improved segmentation precision. Extensive experiments confirm that integrating SAM3 with our adapter yields superior accuracy, robustness, and efficiency compared to all prior SAM-based adaptations. We hope SAM3-Adapter can serve as a foundation for future research and practical segmentation applications. Code, pre-trained models, and data processing pipelines are available.},
	urldate = {2025-12-02},
	publisher = {arXiv},
	author = {Chen, Tianrun and Cao, Runlong and Yu, Xinda and Zhu, Lanyun and Ding, Chaotao and Ji, Deyi and Chen, Cheng and Zhu, Qi and Xu, Chunyan and Mao, Papa and Zang, Ying},
	month = nov,
	year = {2025},
	note = {arXiv:2511.19425 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:/Users/annikaterhoerst/Zotero/storage/XYE9CAUB/Chen et al. - 2025 - SAM3-Adapter Efficient Adaptation of Segment Anything 3 for Camouflage Object Segmentation, Shadow.pdf:application/pdf;Snapshot:/Users/annikaterhoerst/Zotero/storage/HWPGP2YH/2511.html:text/html},
}

@article{itti_model_1998,
	title = {A model of saliency-based visual attention for rapid scene analysis},
	volume = {20},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/document/730558/},
	doi = {10.1109/34.730558},
	abstract = {A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail.},
	number = {11},
	urldate = {2025-12-02},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Itti, L. and Koch, C. and Niebur, E.},
	month = nov,
	year = {1998},
	keywords = {Biological system modeling, Brain modeling, Computer architecture, Feature extraction, Hardware, Image analysis, Layout, Neural networks, Object detection, Visual system},
	pages = {1254--1259},
	file = {Full Text PDF:/Users/annikaterhoerst/Zotero/storage/SLBWYYEK/Itti et al. - 1998 - A model of saliency-based visual attention for rapid scene analysis.pdf:application/pdf},
}

@misc{alexander_kroner_visual_2025,
	title = {Visual {Saliency} {Prediction} - a {Hugging} {Face} {Space} by alexanderkroner},
	url = {https://huggingface.co/spaces/alexanderkroner/saliency},
	abstract = {Upload an image to see where humans are most likely to focus on it. The app highlights key areas of interest based on eye movement data.},
	urldate = {2025-12-02},
	author = {{Alexander Kroner}},
	month = mar,
	year = {2025},
	file = {Snapshot:/Users/annikaterhoerst/Zotero/storage/LJNPQB9I/saliency.html:text/html},
}

@misc{carion_sam_2025,
	title = {{SAM} 3: {Segment} {Anything} with {Concepts}},
	shorttitle = {{SAM} 3},
	url = {http://arxiv.org/abs/2511.16719},
	doi = {10.48550/arXiv.2511.16719},
	abstract = {We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., "yellow school bus"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.},
	urldate = {2025-12-03},
	publisher = {arXiv},
	author = {Carion, Nicolas and Gustafson, Laura and Hu, Yuan-Ting and Debnath, Shoubhik and Hu, Ronghang and Suris, Didac and Ryali, Chaitanya and Alwala, Kalyan Vasudev and Khedr, Haitham and Huang, Andrew and Lei, Jie and Ma, Tengyu and Guo, Baishan and Kalla, Arpit and Marks, Markus and Greer, Joseph and Wang, Meng and Sun, Peize and Rädle, Roman and Afouras, Triantafyllos and Mavroudi, Effrosyni and Xu, Katherine and Wu, Tsung-Han and Zhou, Yu and Momeni, Liliane and Hazra, Rishi and Ding, Shuangrui and Vaze, Sagar and Porcher, Francois and Li, Feng and Li, Siyuan and Kamath, Aishwarya and Cheng, Ho Kei and Dollár, Piotr and Ravi, Nikhila and Saenko, Kate and Zhang, Pengchuan and Feichtenhofer, Christoph},
	month = nov,
	year = {2025},
	note = {arXiv:2511.16719 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/annikaterhoerst/Zotero/storage/TSUSV5UI/Carion et al. - 2025 - SAM 3 Segment Anything with Concepts.pdf:application/pdf;Snapshot:/Users/annikaterhoerst/Zotero/storage/RY6ZLJMJ/2511.html:text/html},
}

@misc{kirillov_segment_2023,
	title = {Segment {Anything}},
	url = {http://arxiv.org/abs/2304.02643},
	doi = {10.48550/arXiv.2304.02643},
	abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
	urldate = {2025-12-03},
	publisher = {arXiv},
	author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollár, Piotr and Girshick, Ross},
	month = apr,
	year = {2023},
	note = {arXiv:2304.02643 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/annikaterhoerst/Zotero/storage/RD352S5K/Kirillov et al. - 2023 - Segment Anything.pdf:application/pdf;Snapshot:/Users/annikaterhoerst/Zotero/storage/7VY5TIK2/2304.html:text/html},
}

@article{qin_u2-net_2020,
	title = {U\${\textasciicircum}2\$-{Net}: {Going} {Deeper} with {Nested} {U}-{Structure} for {Salient} {Object} {Detection}},
	volume = {106},
	issn = {00313203},
	shorttitle = {U\${\textasciicircum}2\$-{Net}},
	url = {http://arxiv.org/abs/2005.09007},
	doi = {10.1016/j.patcog.2020.107404},
	abstract = {In this paper, we design a simple yet powerful deep network architecture, U\${\textasciicircum}2\$-Net, for salient object detection (SOD). The architecture of our U\${\textasciicircum}2\$-Net is a two-level nested U-structure. The design has the following advantages: (1) it is able to capture more contextual information from different scales thanks to the mixture of receptive fields of different sizes in our proposed ReSidual U-blocks (RSU), (2) it increases the depth of the whole architecture without significantly increasing the computational cost because of the pooling operations used in these RSU blocks. This architecture enables us to train a deep network from scratch without using backbones from image classification tasks. We instantiate two models of the proposed architecture, U\${\textasciicircum}2\$-Net (176.3 MB, 30 FPS on GTX 1080Ti GPU) and U\${\textasciicircum}2\$-Net\${\textasciicircum}\{{\textbackslash}dagger\}\$ (4.7 MB, 40 FPS), to facilitate the usage in different environments. Both models achieve competitive performance on six SOD datasets. The code is available: https://github.com/NathanUA/U-2-Net.},
	urldate = {2025-12-03},
	journal = {Pattern Recognition},
	author = {Qin, Xuebin and Zhang, Zichen and Huang, Chenyang and Dehghan, Masood and Zaiane, Osmar R. and Jagersand, Martin},
	month = oct,
	year = {2020},
	note = {arXiv:2005.09007 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {107404},
	file = {Full Text PDF:/Users/annikaterhoerst/Zotero/storage/NFZG8MVH/Qin et al. - 2020 - U\$^2\$-Net Going Deeper with Nested U-Structure for Salient Object Detection.pdf:application/pdf;Snapshot:/Users/annikaterhoerst/Zotero/storage/64LAL9S4/2005.html:text/html},
}

@misc{zhou_image_2024,
	title = {Image {Segmentation} in {Foundation} {Model} {Era}: {A} {Survey}},
	shorttitle = {Image {Segmentation} in {Foundation} {Model} {Era}},
	url = {http://arxiv.org/abs/2408.12957},
	doi = {10.48550/arXiv.2408.12957},
	abstract = {Image segmentation is a long-standing challenge in computer vision, studied continuously over several decades, as evidenced by seminal algorithms such as N-Cut, FCN, and MaskFormer. With the advent of foundation models (FMs), contemporary segmentation methodologies have embarked on a new epoch by either adapting FMs (e.g., CLIP, Stable Diffusion, DINO) for image segmentation or developing dedicated segmentation foundation models (e.g., SAM). These approaches not only deliver superior segmentation performance, but also herald newfound segmentation capabilities previously unseen in deep learning context. However, current research in image segmentation lacks a detailed analysis of distinct characteristics, challenges, and solutions associated with these advancements. This survey seeks to fill this gap by providing a thorough review of cutting-edge research centered around FM-driven image segmentation. We investigate two basic lines of research -- generic image segmentation (i.e., semantic segmentation, instance segmentation, panoptic segmentation), and promptable image segmentation (i.e., interactive segmentation, referring segmentation, few-shot segmentation) -- by delineating their respective task settings, background concepts, and key challenges. Furthermore, we provide insights into the emergence of segmentation knowledge from FMs like CLIP, Stable Diffusion, and DINO. An exhaustive overview of over 300 segmentation approaches is provided to encapsulate the breadth of current research efforts. Subsequently, we engage in a discussion of open issues and potential avenues for future research. We envisage that this fresh, comprehensive, and systematic survey catalyzes the evolution of advanced image segmentation systems. A public website is created to continuously track developments in this fast advancing field: {\textbackslash}url\{https://github.com/stanley-313/ImageSegFM-Survey\}.},
	urldate = {2025-12-03},
	publisher = {arXiv},
	author = {Zhou, Tianfei and Xia, Wang and Zhang, Fei and Chang, Boyu and Wang, Wenguan and Yuan, Ye and Konukoglu, Ender and Cremers, Daniel},
	month = nov,
	year = {2024},
	note = {arXiv:2408.12957 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:/Users/annikaterhoerst/Zotero/storage/6YDFQNCF/Zhou et al. - 2024 - Image Segmentation in Foundation Model Era A Survey.pdf:application/pdf;Snapshot:/Users/annikaterhoerst/Zotero/storage/BCRW5SSV/2408.html:text/html},
}

@misc{zhou_image_2024-1,
	title = {Image {Segmentation} in {Foundation} {Model} {Era}: {A} {Survey}},
	shorttitle = {Image {Segmentation} in {Foundation} {Model} {Era}},
	url = {http://arxiv.org/abs/2408.12957},
	doi = {10.48550/arXiv.2408.12957},
	abstract = {Image segmentation is a long-standing challenge in computer vision, studied continuously over several decades, as evidenced by seminal algorithms such as N-Cut, FCN, and MaskFormer. With the advent of foundation models (FMs), contemporary segmentation methodologies have embarked on a new epoch by either adapting FMs (e.g., CLIP, Stable Diffusion, DINO) for image segmentation or developing dedicated segmentation foundation models (e.g., SAM). These approaches not only deliver superior segmentation performance, but also herald newfound segmentation capabilities previously unseen in deep learning context. However, current research in image segmentation lacks a detailed analysis of distinct characteristics, challenges, and solutions associated with these advancements. This survey seeks to fill this gap by providing a thorough review of cutting-edge research centered around FM-driven image segmentation. We investigate two basic lines of research -- generic image segmentation (i.e., semantic segmentation, instance segmentation, panoptic segmentation), and promptable image segmentation (i.e., interactive segmentation, referring segmentation, few-shot segmentation) -- by delineating their respective task settings, background concepts, and key challenges. Furthermore, we provide insights into the emergence of segmentation knowledge from FMs like CLIP, Stable Diffusion, and DINO. An exhaustive overview of over 300 segmentation approaches is provided to encapsulate the breadth of current research efforts. Subsequently, we engage in a discussion of open issues and potential avenues for future research. We envisage that this fresh, comprehensive, and systematic survey catalyzes the evolution of advanced image segmentation systems. A public website is created to continuously track developments in this fast advancing field: {\textbackslash}url\{https://github.com/stanley-313/ImageSegFM-Survey\}.},
	urldate = {2025-12-03},
	publisher = {arXiv},
	author = {Zhou, Tianfei and Xia, Wang and Zhang, Fei and Chang, Boyu and Wang, Wenguan and Yuan, Ye and Konukoglu, Ender and Cremers, Daniel},
	month = nov,
	year = {2024},
	note = {arXiv:2408.12957 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/annikaterhoerst/Zotero/storage/YQ8XBQC2/Zhou et al. - 2024 - Image Segmentation in Foundation Model Era A Survey.pdf:application/pdf},
}

@misc{ren_grounded_2024,
	title = {Grounded {SAM}: {Assembling} {Open}-{World} {Models} for {Diverse} {Visual} {Tasks}},
	shorttitle = {Grounded {SAM}},
	url = {http://arxiv.org/abs/2401.14159},
	doi = {10.48550/arXiv.2401.14159},
	abstract = {We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM). This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models. As shown in Fig.1, a wide range of vision tasks can be achieved by using the versatile Grounded SAM pipeline. For example, an automatic annotation pipeline based solely on input images can be realized by incorporating models such as BLIP and Recognize Anything. Additionally, incorporating Stable-Diffusion allows for controllable image editing, while the integration of OSX facilitates promptable 3D human motion analysis. Grounded SAM also shows superior performance on open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in the wild) zero-shot benchmark with the combination of Grounding DINO-Base and SAM-Huge models.},
	urldate = {2025-12-04},
	publisher = {arXiv},
	author = {Ren, Tianhe and Liu, Shilong and Zeng, Ailing and Lin, Jing and Li, Kunchang and Cao, He and Chen, Jiayu and Huang, Xinyu and Chen, Yukang and Yan, Feng and Zeng, Zhaoyang and Zhang, Hao and Li, Feng and Yang, Jie and Li, Hongyang and Jiang, Qing and Zhang, Lei},
	month = jan,
	year = {2024},
	note = {arXiv:2401.14159 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/annikaterhoerst/Zotero/storage/VBSRCY5Q/Ren et al. - 2024 - Grounded SAM Assembling Open-World Models for Diverse Visual Tasks.pdf:application/pdf;Snapshot:/Users/annikaterhoerst/Zotero/storage/CK4DE8Q4/2401.html:text/html},
}

@misc{xiao_florence-2_2023,
	title = {Florence-2: {Advancing} a {Unified} {Representation} for a {Variety} of {Vision} {Tasks}},
	shorttitle = {Florence-2},
	url = {http://arxiv.org/abs/2311.06242},
	doi = {10.48550/arXiv.2311.06242},
	abstract = {We introduce Florence-2, a novel vision foundation model with a unified, prompt-based representation for a variety of computer vision and vision-language tasks. While existing large vision models excel in transfer learning, they struggle to perform a diversity of tasks with simple instructions, a capability that implies handling the complexity of various spatial hierarchy and semantic granularity. Florence-2 was designed to take text-prompt as task instructions and generate desirable results in text forms, whether it be captioning, object detection, grounding or segmentation. This multi-task learning setup demands large-scale, high-quality annotated data. To this end, we co-developed FLD-5B that consists of 5.4 billion comprehensive visual annotations on 126 million images, using an iterative strategy of automated image annotation and model refinement. We adopted a sequence-to-sequence structure to train Florence-2 to perform versatile and comprehensive vision tasks. Extensive evaluations on numerous tasks demonstrated Florence-2 to be a strong vision foundation model contender with unprecedented zero-shot and fine-tuning capabilities.},
	urldate = {2025-12-04},
	publisher = {arXiv},
	author = {Xiao, Bin and Wu, Haiping and Xu, Weijian and Dai, Xiyang and Hu, Houdong and Lu, Yumao and Zeng, Michael and Liu, Ce and Yuan, Lu},
	month = nov,
	year = {2023},
	note = {arXiv:2311.06242 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/annikaterhoerst/Zotero/storage/QTY3A6H2/Xiao et al. - 2023 - Florence-2 Advancing a Unified Representation for a Variety of Vision Tasks.pdf:application/pdf;Snapshot:/Users/annikaterhoerst/Zotero/storage/PA8YNZG3/2311.html:text/html},
}

@misc{zou_segment_2023,
	title = {Segment {Everything} {Everywhere} {All} at {Once}},
	url = {http://arxiv.org/abs/2304.06718},
	doi = {10.48550/arXiv.2304.06718},
	abstract = {In this work, we present SEEM, a promptable and interactive model for segmenting everything everywhere all at once in an image, as shown in Fig.1. In SEEM, we propose a novel decoding mechanism that enables diverse prompting for all types of segmentation tasks, aiming at a universal segmentation interface that behaves like large language models (LLMs). More specifically, SEEM is designed with four desiderata: i) Versatility. We introduce a new visual prompt to unify different spatial queries including points, boxes, scribbles and masks, which can further generalize to a different referring image; ii) Compositionality. We learn a joint visual-semantic space between text and visual prompts, which facilitates the dynamic composition of two prompt types required for various segmentation tasks; iii) Interactivity. We further incorporate learnable memory prompts into the decoder to retain segmentation history through mask-guided cross-attention from decoder to image features; and iv) Semantic-awareness. We use a text encoder to encode text queries and mask labels into the same semantic space for open-vocabulary segmentation. We conduct a comprehensive empirical study to validate the effectiveness of SEEM across diverse segmentation tasks. Notably, our single SEEM model achieves competitive performance across interactive segmentation, generic segmentation, referring segmentation, and video object segmentation on 9 datasets with minimum 1/100 supervision. Furthermore, SEEM showcases a remarkable capacity for generalization to novel prompts or their combinations, rendering it a readily universal image segmentation interface.},
	urldate = {2025-12-04},
	publisher = {arXiv},
	author = {Zou, Xueyan and Yang, Jianwei and Zhang, Hao and Li, Feng and Li, Linjie and Wang, Jianfeng and Wang, Lijuan and Gao, Jianfeng and Lee, Yong Jae},
	month = jul,
	year = {2023},
	note = {arXiv:2304.06718 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/annikaterhoerst/Zotero/storage/FRSJZ5NY/Zou et al. - 2023 - Segment Everything Everywhere All at Once.pdf:application/pdf;Snapshot:/Users/annikaterhoerst/Zotero/storage/AZWPURUQ/2304.html:text/html},
}
