% Local IspellDict: en
\chapter{Theoretical Background}\label{ch02:theoretical}

\section{Image Segmentation}

Image segmentation is the process of dividing an image into different regions by grouping pixels and assigning each pixel a label. 
This step is an important part of many computer vision applications, such as detecting tumors in medical images or identifying 
pedestrians in autonomous driving. According to human visual perception, the identified regions are non-overlapping and meaningful 
- however, defining what exactly counts as a “meaningful” region can be difficult, as human perception is subjective and 
object boundaries are not always clear \citep{yu_techniques_2023}.

There are three common types of segmentation:

\textit{Semantic segmentation} assigns every pixel in an image a semantic label, such as “car” or “sky”.
\textit{Instance segmentation} separates individual objects within the same class, for example distinguishing several people in one image.
\textit{Panoptic segmentation} combines both approaches by providing pixel-wise class labels and also identifying individual object instances.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/segmentation_types.png}
    \caption{Types of image segmentation by \cite{kirillov_panoptic_2019}}
\end{figure}

Earlier approaches to image segmentation include algorithms such as k-means-clustering \citep{dhanachandra_image_2015}. Yet in recent years, deep learning models 
have significantly improved the segmentation effect and performance, therefore becoming the dominant method for solving segmentation 
tasks in complex environments \citep{minaee_image_2022}.

According to \cite{zhou_image_2024-1}, the above-described image segmentation methods fall into the category of generic image segmentation (GIS). 
The category of promptable image segmentation (PIS) extends GIS by specifying the target to segment through a prompt. 
This prompt can have various forms such as text, box or points.

\section{Salient Object Detection}
The human visual system pays more attention to certain parts in an image, a property known as saliency. 
Inspired by this mechanism, \textit{saliency detection} models aim to predict which regions in an image are most likely to attract human visual attention. 
These models typically provide saliency maps in form of heat maps, in which higher intensity values indicate regions detected to be more important \citep{ahmadi_context_2018}.

\textit{Salient Object Detection (SOD)} – also referred to as salient object segmentation \citep{borji_salient_2019} or saliency segmentation \citep{kakanopas_unified_2021} – 
goes one step further by segmenting the most salient object(s) of an image. SOD can be interpreted as a two-stage process: 
1) Detection of the most salient object and 2) Accurate segmentation of the region of that object. 
In contrast to general image segmentation, SOD focuses on segmenting only those objects that are (or that are predicted to be) most salient \citep{liu_learning_2011, borji_salient_2019}. 
Figure \ref{fig:sod_example} illustrates the difference between saliency detection and salient object detection.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\linewidth]{figures/original_image_statue.jpg}
        \caption{}
        \label{fig:a}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\linewidth]{figures/saliency_detection_example.jpeg}
        \caption{}
        \label{fig:b}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\linewidth]{figures/sod_examlple.png}
        \caption{}
        \label{fig:c}
    \end{subfigure}

    \caption{(a) the original image, (b) saliency map \citep{alexander_kroner_visual_2025} (c) salient object detection mask generated using SAM3 guided by the eye-tracking data}
    \label{fig:sod_example}
\end{figure}



\section{Image Segmentation Models}

Table \ref{fig:segmentation_models} offers a structured overview of prominent and state-of-the-art image segmentation models, 
organized according to the segmentation tasks for which they are most suitable 
\footnote{Note: The listed models were identified through a combination of literature searches and exploratory queries using Google and ChatGPT to ensure the inclusion of both widely recognized and up-to-date segmentation approaches.}.

\begin{figure}[b]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/screenshot models.png}
    \caption{Overview of prominent image segmentation models}
    \label{fig:segmentation_models}
\end{figure}

For our use-case, only promptable segmentation models are suitable as the eye-tracking data provides points which will be used as input to implement SOD. 
Among the promptable models, SAM (and its successors) is the only widely used model that natively supports point prompts. 
Grounded-SAM and Florence-2 are only text-promptable and SEEM is - with the last update in 2023 - less actively maintained and used than SAM.


The Segment Anything Model is developed by Meta AI and was first introduced in mid-2023 \citep{kirillov_segment_2023}. 
SAM segments objects using prompts such as points and boxes.  With over 15.000 citations, SAM is de facto standard for domain specific implementations 
and is already being used for other specific use cases of salient object detection: \cite{chen_sam3-adapter_2025,liu_samsod_2025,yuan_saliencyclip-sam_2026}. 
he newest release, SAM3, was on the 20th of November 2025. By reaching over 5.1k stars on GitHub within only 2 weeks, it indicates strong community interest. 
Compared to its predecessors, SAM 3 introduces the ability to detect and segment instances matching a given text description, 
and to further refine detections using visual examples \citep{carion_sam_2025}.

\textcolor{red}{TODO: segmentation models schöner schreiben, teilweise noch quellen ergänzen}

\textcolor{red}{TODO IDEA: noch kurz was zur EVALUATION von segmentation models schreiben! also IoU, dice coefficent, precision/recall, F-measure, MAE, etc. wenigstens kurz erwähnen}

\textcolor{blue}{notiz motivation?: we want to try out the combination of real eye-tracking data on social media images with SAM to achieve salient object detection in this specific use-case and context}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main_thesis"
%%% TeX-command-extra-options: "-shell-escape"
%%% End:
